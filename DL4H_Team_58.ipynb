{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "dlv6knX04FiY"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "The paper we have selected is focused on Cross Architectural Self-Supervision for Healthcare Applications.\n",
        "\n",
        "The relevant code can be found in this repository: https://github.com/dl4h/project-team-58\n",
        "\n",
        "The Colab notebook and relevant datasets and checkpoints can be found here: https://drive.google.com/drive/folders/1bD-T5_J6JfuNw8e0VWO74tLt1LPa9aw8?usp=sharing\n",
        "\n",
        "## Background\n",
        "* **Type of Problem** The major type of problem being solved by CASS is related to representation learning and data processing:\n",
        "  - Existing self-supervised learning models often require expensive computational resources not widely available.\n",
        "  - Medical imaging and Artificial Intelligence is often limited by a scarcity in prelabeled training data.\n",
        "\n",
        "  To mitigate these issues, the authors propose *CASS* which allows for training CNN's and Transformers which can reduce pretraining time using less data and less computing resources.\n",
        "\n",
        "  **Importance**: Representation learning allows us to use self-supervision to learn useful priors by pretraining unlabeled images. This is crucially important because the medical imaging field suffers from a lack of available labeled data due to a variety of factors, such as the high cost of labeling data at scale because it generally requires domain-specific knowledge. In fields with limited data or high cost to produce labels, we can use self-supervision to help with the downstream learning process without the need for labels. Solving these issues is important because it might allow for the expansion of machine learning and artificial intelligence research into more scenarios which might have previously been hindered by a lack of data or computing resources.\n",
        "\n",
        "  **Difficulty**: Existing state-of-the-art self-supervised learning methods have extreme computational requirements that make them inaccessible to most practitioners. Additionally, the limited amount of data makes it infeasible to run smaller epochs with larger batch size to achieve the effectiveness outlined in these self-supervised learning methods.\n",
        "  Overcoming the problem is difficult in that it is often a matter of logistics and practicality. For example, gaining labeled data might be impossible for a relatively new disease like COVID-19, as the authors describe in their paper.\n",
        "  \n",
        "  **State of the art methods and effectiveness**: As detailed above, existing state of the art methods for representation learning face challenges due to significant computational requirements and limited data availability. Traditionally, contrastive self-supervision methods use different augmentations of images to create positive pairs. Because of this, augmentations are applied twice which increases time complexity overall. Additionally, parameter sharing between the two architectures increases the time complexity when re-initializing architectures with lagging parameters. Additionally, smaller epochs and batch sizes tend to hurt performance.The current \"state of the art\" in terms of broad use is *Transfer Learning*, where a model developed for one task is reused as the starting point for a model on a second task. This is practical for scenarios in which there is a lack of data. The authors note that this is common in medical research due to issues such as patient privact or disease prevalence. However, CASS represents a different approach known as *Self-Supervised Learning*.\n",
        "\n",
        "# Paper Explanation\n",
        "\n",
        "## The Proposal\n",
        "In the paper, the authors acknowledge that self-supervisied learning is generally superior to Transfer Learning, but they require multiple advanced graphical processing units (GPU's) running over the course of several days, something many researchers might consider a luxury both in terms of time and money.\n",
        "Additionally, many self-supervised models suffer in terms of performance when run with small batch sizes.\n",
        "\n",
        "To this end, the authors propose combining a convolutional neural network (CNN) with a transformer in a \"response-based siamese contrastive method\".\n",
        "\n",
        "## The Innovations\n",
        "\n",
        "CASS helps solve our general problem while mitigating these challenges by leveraging CNN and Transformer methods for efficient learning in a siamese contrastive method. CASS leverages architecture invariance instead of using the augmentation invariance approach of existing methods. Extracted representations of each input image are compared across two branches representing each respective architecture. By contrasting their extracted features, they can learn from each other on patterns they would generally miss. This helps provide more useful pre-trained data for the downstream learning method.\n",
        "\n",
        "## Metrics (How Well it Worked)\n",
        "CASS's approach helps reduce the time complexity of pre-training in two major ways. First, augmentations are only applied once in CASS in comparison to twice in augmentation invariance approaches. Therefore per application CASS uses less augmentations overall. Second, there is no scope for parameter sharing in CASS because the two architectures used are different. A large portion of time is saved in updating the two architectures each epoch as opposed to re-initializing architectures with lagging parameters. CASS has also been proven to handle smaller epochs and batch sizes with better performance overall.\n",
        "\n",
        "## Contribution to Research Regime\n",
        "The contribution is extremely important to the research regime. Without CASS certain representation learning problems would not be feasible to solve because of computational requirements and data availability. CASS overcomes those challenges while achieving even better performance.\n",
        "CASS is recognized as a cutting-edged self-supervision learning method with accolades advertised on its' Github page such as:\n",
        "- *State of the Art*: Partial Label Learning on Autoimmune Dataset\n",
        "- *State of the Art*: Classification on Brain Tumor MRI Dataset\n",
        "- *State of the Art*: State of the Art: Classification on ISIC 2019\n"
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code comment is used as inline annotations for your coding"
      ],
      "metadata": {
        "id": "ABD4VhFZbehA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility\n",
        "\n",
        "\n",
        "1. **Hypothesis 1**: Leveraging the CASS self-supervised learning approach will significantly improve the efficiency of representation learning in healthcare applications in scenarios which involve a lack of data or computing resources\n",
        "\n",
        "\n",
        "2. **Hypothesis 2 (Ablation study)**: Reducing the number of pre-training epochs and batch sizes for the CASS model will still allow for strong model performance in comparison with existing methods. **Note: As of final submission, this could not be accomplished.**\n"
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Notebook to Google Drive\n",
        "Upload the data, pretrianed model, figures, etc to your Google Drive, then mount this notebook to Google Drive. After that, you can access the resources freely.\n",
        "\n",
        "Instruction: https://colab.research.google.com/notebooks/io.ipynb\n",
        "\n",
        "Example: https://colab.research.google.com/drive/1srw_HFWQ2SMgmWIawucXfusGzrj1_U0q\n",
        "\n",
        "Video: https://www.youtube.com/watch?v=zc8g8lGcwQU"
      ],
      "metadata": {
        "id": "dlv6knX04FiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "sfk8Zrul_E8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n",
        "\n",
        "The original CASS authors utilized the following datasets in their paper:\n",
        "- DERMOFIT\n",
        "- Brain MRI Classification\n",
        "- SIIM-ISIC 2019 Dataset\n",
        "\n",
        "The paper involved training a CNN and Transformer using CASS, then evaluating their performance vs. DINO (an alternative state-of-the-art self-supervision model).\n",
        "\n",
        "However, in our approach we found that only the Brain MRI Classification dataset was available (with further confusing findings). The [official CASS Github repository](https://github.com/pranavsinghps1/CASS) contains the code required for training CASS on a given CNN and attention model specifically in the case of a fourth dataset: [MedMNIST](https://medmnist.com/). Specifically, PathMNIST which is a multi-class dataset for Colon pathology.\n",
        "\n",
        "Based on these findings, we opted to attempt to recreate the CASS paper using the Brain Tumor dataset and MedMNIST.\n",
        "\n",
        "We recognized that the MedMNIST dataset was not originally used in the dataset, but in our proposal we originally suggested that we may be able to leverage this dataset to test our hypothesis (*Leveraging CASS self-supervised learning will significantly improve efficiency of representation learning in scenarios which involve a lack of healthcare data or computing resources*)."
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment\n",
        "\n",
        "We utilized Google Colab to run our experiment notebook. The python version and associated libraries are detailed in the cells below."
      ],
      "metadata": {
        "id": "o8Bh7i_zWJKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(\"Python version:\", sys.version)"
      ],
      "metadata": {
        "id": "HohG2h57WIbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Required Installs"
      ],
      "metadata": {
        "id": "vV01FvZeEtyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install necessary libraries (from requirements.txt) TODO Note errors in output\n",
        "%pip install einops~=0.4.1\n",
        "%pip install matplotlib~=3.5.2\n",
        "%pip install matplotlib-inline~=0.1.2\n",
        "%pip install numpy~=1.23.1\n",
        "%pip install pandas~=1.4.3\n",
        "%pip install Pillow~=9.2.0\n",
        "%pip install scikit-learn~=1.1.1\n",
        "%pip install scipy~=1.8.1\n",
        "%pip install tensorboard~=2.9.1\n",
        "%pip install timm~=0.5.4\n",
        "%pip install torch~=1.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
        "%pip install torchtext~=0.12.0\n",
        "%pip install torchaudio~=0.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
        "%pip install torchcontrib~=0.0.2\n",
        "%pip install torchmetrics~=0.9.2\n",
        "%pip install torchvision~=0.12.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
        "%pip install vit-pytorch~=0.35.8\n",
        "%pip install pytorch-lightning~=1.6.5\n",
        "%pip install tqdm~=4.64.0\n",
        "%pip install h5py # for loading brain tumor HDF5 dataset\n",
        "%pip install medmnist\n",
        "\n",
        "# import packages you need\n",
        "# from google.colab import drive\n",
        "\n",
        "# imports for CASS\n",
        "import math\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pytorch_lightning as pl\n",
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pytorch_lightning import Trainer, seed_everything\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchcontrib.optim import SWA\n",
        "from torchmetrics import Metric\n",
        "from torchvision import transforms as tsfm\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Brain Tumor Dataset"
      ],
      "metadata": {
        "id": "mQjsgxn2g1IL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data\n",
        "\n",
        "The authors cite the brain tumor dataset from their official Github:\n",
        "1. https://figshare.com/articles/dataset/brain_tumor_dataset/1512427\n",
        "2. https://www.hindawi.com/journals/cin/2022/3236305/\n",
        "\n",
        "Where #1 links to a dataset which hosts the images in zipped matlab format. Link #2 references another research paper, which itself references the Brain Tumor dataset located on [Kaggle](https://www.kaggle.com/datasets/sartajbhuvaji/brain-tumor-classification-mri/).\n",
        "\n",
        "The Kaggle version of the dataset includes a reference to the [Github version of the same dataset](https://github.com/SartajBhuvaji/Brain-Tumor-Classification-Using-Deep-Learning-Algorithms).\n",
        "\n",
        "For our experiment, we opted to use the Github version of the dataset which provided the below starter code for loading the dataset as well as defining train and test data. It is important to note that the CASS authors also describe this data as having been split (train/test) in this same way by the data curators. As such, we did not change these definitions.\n",
        "\n",
        "- The methods for cloning and loading a train/test dataset were re-used from that Github repository, however the attempted CASS implementation and exploratory data analysis (EDA) remains the work of this team."
      ],
      "metadata": {
        "id": "_klTgh4gc1gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The code below is from the brain tumor notebook associated with its' official Github/Kaggle repository, and is only for\n",
        "# retrieving and setting up train/test data.\n",
        "# The CASS authors mention that the brain tumor dataset they used (and this is the same one they referenced) are pre-split\n",
        "\n",
        "import cv2\n",
        "import random\n",
        "import pickle\n",
        "import tqdm\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "!git clone https://github.com/SartajBhuvaji/Brain-Tumor-Classification-DataSet\n",
        "\n",
        "# Define necessary constants\n",
        "IS_LOCAL = True\n",
        "BASE_PATH = './' if IS_LOCAL else '/content'\n",
        "TEST_DIR = f'{BASE_PATH}Brain-Tumor-Classification-DataSet/Testing'\n",
        "TRAIN_DIR = f'{BASE_PATH}Brain-Tumor-Classification-DataSet/Training'\n",
        "IMG_SIZE = 224\n",
        "CATEGORIES = [\"glioma_tumor\",\"meningioma_tumor\",\"no_tumor\",\"pituitary_tumor\"]\n",
        "\n",
        "# Creating training dataset\n",
        "training_data = []\n",
        "\n",
        "def create_training_data():\n",
        "    for category in CATEGORIES:\n",
        "        path = os.path.join(TRAIN_DIR,category)\n",
        "        class_num = CATEGORIES.index(category)\n",
        "        for img in tqdm.tqdm(os.listdir(path)):\n",
        "          img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_COLOR)\n",
        "          new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
        "          training_data.append([new_array, class_num])\n",
        "\n",
        "    random.shuffle(training_data)\n",
        "\n",
        "create_training_data()\n",
        "#np.save('train_data.npy', training_data)\n",
        "print(f'training dataset size: {len(training_data)}')\n",
        "\n",
        "X_train = np.array([i[0] for i in training_data]).reshape(-1,IMG_SIZE,IMG_SIZE,3)\n",
        "Y_train = [i[1] for i in training_data]\n",
        "\n",
        "pickle_out = open(\"X_train.pickle\",\"wb\")\n",
        "pickle.dump(X_train, pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "pickle_out = open(\"Y_train.pickle\",\"wb\")\n",
        "pickle.dump(Y_train, pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "\n",
        "# Creating testing dataset\n",
        "testing_data = []\n",
        "\n",
        "def create_testing_data():\n",
        "    for category in CATEGORIES:\n",
        "        path = os.path.join(TEST_DIR,category)\n",
        "        class_num = CATEGORIES.index(category)\n",
        "\n",
        "        for img in tqdm.tqdm(os.listdir(path)):\n",
        "          img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_COLOR)\n",
        "          new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
        "          testing_data.append([new_array, class_num])\n",
        "\n",
        "    random.shuffle(testing_data)\n",
        "\n",
        "create_testing_data()\n",
        "#np.save('testing_data.npy', testing_data)\n",
        "print(f'testing dataset size: {len(testing_data)}')\n",
        "X_test= np.array([i[0] for i in testing_data]).reshape(-1,IMG_SIZE,IMG_SIZE,3)\n",
        "Y_test = [i[1] for i in testing_data]\n",
        "\n",
        "pickle_out = open(\"X_test.pickle\",\"wb\")\n",
        "pickle.dump(X_test, pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "pickle_out = open(\"Y_test.pickle\",\"wb\")\n",
        "pickle.dump(Y_test, pickle_out)\n",
        "pickle_out.close()"
      ],
      "metadata": {
        "id": "nzCPd73RGVnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exploratory Data Analysis (Brain Tumor Dataset)\n",
        "Here, we analyze the data to understand it's format and most importantly, to understand if it is the same data the authors describe."
      ],
      "metadata": {
        "id": "R4-ryVzpGtz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the training or testing images and labels\n",
        "pickle_in = open(\"X_train.pickle\", \"rb\")\n",
        "X_train = pickle.load(pickle_in)\n",
        "pickle_in.close()\n",
        "\n",
        "pickle_in = open(\"Y_train.pickle\", \"rb\")\n",
        "Y_train = pickle.load(pickle_in)\n",
        "pickle_in.close()\n",
        "\n",
        "# Choose a sample image and label, for example, the first one in the dataset\n",
        "sample_image = X_train[0]\n",
        "sample_label = Y_train[0]\n",
        "\n",
        "# Since the images are stored in BGR format by OpenCV, convert them to RGB for displaying\n",
        "sample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(sample_image)\n",
        "plt.title(f'Tumor Type: {sample_label}')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jSMD1BGpHOWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# understand distribution of labels\n",
        "\n",
        "with open(\"Y_train.pickle\", \"rb\") as f:\n",
        "    Y_train = pickle.load(f)\n",
        "\n",
        "with open(\"Y_test.pickle\", \"rb\") as f:\n",
        "    Y_test = pickle.load(f)\n",
        "\n",
        "all_labels = Y_train + Y_test\n",
        "\n",
        "tumor_types = {'glioma_tumor': 0, 'meningioma_tumor': 0, 'no_tumor': 0, 'pituitary_tumor': 0}\n",
        "\n",
        "for label in all_labels:\n",
        "    category = CATEGORIES[label]\n",
        "    if category in tumor_types:\n",
        "        tumor_types[category] += 1\n",
        "\n",
        "# visualization\n",
        "plt.bar(tumor_types.keys(), tumor_types.values())\n",
        "plt.xlabel('Tumor Type')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Tumor Types')\n",
        "plt.xticks(rotation=45)  # Rotate category names for better visibility\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "maThAOfIHPVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Statement Regarding Brain Tumor Dataset\n",
        "\n",
        "The authors describe in their paper a brain tumor dataset which has 5,712 training images and 1,310 test images which results in approximately 18.66% train data in their experiment, with the remaining percentage allocated to test. This is something close to a 80/20 train test split.\n",
        "\n",
        "However, the authors of this project found multiple instances of the brain tumor dataset (both referenced in the official CASS Github), and in both cases the number of total images (as well as train vs. test is different).\n",
        "\n",
        "In our case, we have 3,264 total images, with 2,870 for training and 394 for testing. Applying the same understanding as above, this means we have a test set ratio of ~ 12.07% to training data of 87.93%.\n",
        "\n",
        "**Because we are not operating with the same total data (or ratios), we cannot accurately replicate the study as intended using this dataset. We attempted to load this data into the CNN/Attention modle using the CASS-supplied code but ultimately were unsuccessful. These efforts will be explained in the Results section.**"
      ],
      "metadata": {
        "id": "yhonyosFG-oz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MedMNIST"
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Data\n",
        "\n",
        "\n",
        "  The data source for our project is the PathMNIST dataset located within the larger MedMNIST database. The MedMNIST data is curated by researchers from several universities, such as: Shanghai Jiao Tong University, RWTH Aachen University, and Harvard University.\n",
        "\n",
        "  PathMNIST in particular is a collection of images corresponding to Colon Pathology with regard to (9) different classes or \"labels\".\n",
        "\n",
        "  The dataset is located here: https://zenodo.org/records/10519652 but for our purposes, we chose to utilize the Python package (https://pypi.org/project/medmnist/).\n",
        "\n",
        "  * Statistics: include basic descriptive statistics of the dataset like size, cross validation split, label distribution, etc.\n",
        "  * Data process: how do you munipulate the data, e.g., change the class labels, split the dataset to train/valid/test, refining the dataset."
      ],
      "metadata": {
        "id": "9dU7t36jcjVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as data\n",
        "import medmnist\n",
        "from medmnist import INFO\n",
        "\n",
        "data_flag = \"pathmnist\"\n",
        "download = True\n",
        "\n",
        "# BATCH_SIZE = 128 --> this causes Colab machines to run out of memory, even with V100 GPU\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "\n",
        "# Load dataset information\n",
        "info = INFO[data_flag]\n",
        "n_channels = info[\"n_channels\"]\n",
        "n_classes = len(info[\"label\"])\n",
        "\n",
        "DataClass = getattr(medmnist, info[\"python_class\"])\n",
        "\n",
        "# NOTE switched code below to what is used in the CASS.ipynb medmnist example instead of MNIST Get-started-CASS.ipynb\n",
        "# Define transformations\n",
        "# possibly overkill for EDA\n",
        "\"\"\"\n",
        "Define train & valid image transformation\n",
        "\"\"\"\n",
        "DATASET_IMAGE_MEAN = (0.485, 0.456, 0.406)\n",
        "DATASET_IMAGE_STD = (0.229, 0.224, 0.225)\n",
        "\n",
        "train_transform = tsfm.Compose(\n",
        "    [\n",
        "        tsfm.Resize((384, 384)),\n",
        "        tsfm.RandomApply(\n",
        "            [\n",
        "                tsfm.ColorJitter(0.2, 0.2, 0.2),\n",
        "                tsfm.RandomPerspective(distortion_scale=0.2),\n",
        "            ],\n",
        "            p=0.3,\n",
        "        ),\n",
        "        tsfm.RandomApply(\n",
        "            [\n",
        "                tsfm.RandomAffine(degrees=10),\n",
        "            ],\n",
        "            p=0.3,\n",
        "        ),\n",
        "        tsfm.RandomVerticalFlip(p=0.3),\n",
        "        tsfm.RandomHorizontalFlip(p=0.3),\n",
        "        tsfm.ToTensor(),\n",
        "        tsfm.Normalize(DATASET_IMAGE_MEAN, DATASET_IMAGE_STD),\n",
        "    ]\n",
        ")\n",
        "\n",
        "valid_transform = tsfm.Compose(\n",
        "    [\n",
        "        tsfm.Resize((384, 384)),\n",
        "        tsfm.ToTensor(),\n",
        "        tsfm.Normalize(DATASET_IMAGE_MEAN, DATASET_IMAGE_STD),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Load the data\n",
        "train_dataset = DataClass(split=\"train\", transform=train_transform, download=download)\n",
        "val_dataset = DataClass(split=\"val\", transform=valid_transform, download=download)\n",
        "test_dataset = DataClass(split=\"test\", transform=valid_transform, download=download)\n",
        "\n",
        "#pil_dataset = DataClass(split=\"train\", download=download)\n",
        "\n",
        "# encapsulate data into dataloader form\n",
        "train_samples = 840\n",
        "train_sampler = data.RandomSampler(train_dataset, num_samples=train_samples)\n",
        "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
        "#train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "val_samples = 90\n",
        "val_sampler = data.RandomSampler(val_dataset, num_samples=val_samples)\n",
        "valid_loader = data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler)\n",
        "#valid_loader = data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "train_loader_at_eval = data.DataLoader(dataset=train_dataset, batch_size=2*BATCH_SIZE, sampler=train_sampler)\n",
        "#train_loader_at_eval = data.DataLoader(dataset=train_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
        "\n",
        "test_samples = 70\n",
        "test_sampler = data.RandomSampler(test_dataset, num_samples=test_samples)\n",
        "test_loader = data.DataLoader(dataset=test_dataset, batch_size=2*BATCH_SIZE, sampler=test_sampler)\n",
        "#test_loader = data.DataLoader(dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(train_dataset)\n",
        "print(\"===================\")\n",
        "print(test_dataset)"
      ],
      "metadata": {
        "id": "SYKkVOzSjUCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summary statistics\n",
        "import pandas as pd\n",
        "\n",
        "# create a dataframe with summary statistics\n",
        "data = {\n",
        "    \"Dataset\": [\"train\", \"validation\", \"test\"],\n",
        "    \"Number of images\": [len(train_dataset), len(val_dataset), len(test_dataset)],\n",
        "}\n",
        "\n",
        "# Include total sum\n",
        "total_images = sum(data[\"Number of images\"])\n",
        "data[\"Dataset\"].append(\"Total\")\n",
        "data[\"Number of images\"].append(total_images)\n",
        "\n",
        "summary_df = pd.DataFrame(data)\n",
        "summary_df"
      ],
      "metadata": {
        "id": "dtPal8m7nWTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZScZNbROw-N"
      },
      "outputs": [],
      "source": [
        "# Show sample images in this cell\n",
        "print('Sample training images')\n",
        "train_dataset.montage()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show class distribution of labels\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# get the labels\n",
        "train_labels = train_dataset.labels\n",
        "val_labels = val_dataset.labels\n",
        "test_labels = test_dataset.labels\n",
        "\n",
        "# Map label indices to descriptive names\n",
        "label_names = info[\"label\"]\n",
        "\n",
        "# create a dataframe with the labels\n",
        "train_labels_df = pd.DataFrame(train_labels, columns=[\"label\"])\n",
        "train_labels_df[\"label_name\"] = train_labels_df[\"label\"].map(label_names)\n",
        "val_labels_df = pd.DataFrame(val_labels, columns=[\"label\"])\n",
        "val_labels_df[\"label_name\"] = val_labels_df[\"label\"].map(label_names)\n",
        "test_labels_df = pd.DataFrame(test_labels, columns=[\"label\"])\n",
        "test_labels_df[\"label_name\"] = test_labels_df[\"label\"].map(label_names)\n",
        "\n",
        "# plot the class distribution\n",
        "fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
        "train_labels_df[\"label\"].value_counts().plot(kind=\"bar\", ax=ax[0], title=\"Train Labels\")\n",
        "val_labels_df[\"label\"].value_counts().plot(\n",
        "    kind=\"bar\", ax=ax[1], title=\"Validation Labels\"\n",
        ")\n",
        "test_labels_df[\"label\"].value_counts().plot(kind=\"bar\", ax=ax[2], title=\"Test Labels\")\n",
        "\n",
        "for i in range(3):\n",
        "    # Append x-axis labels with descriptions\n",
        "    ax[i].set_xticklabels(\n",
        "        [\n",
        "            f\"{label_names[t.get_text()]} ({t.get_text()})\"\n",
        "            for t in ax[i].get_xticklabels()\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Slighly rotate x-axis labels for better readability\n",
        "    plt.setp(ax[i].get_xticklabels(), rotation=45, ha=\"right\")\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Eaxcbk9InR4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "hEpOyAfbp5X4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "\n",
        "# # datasets_path = '/content/drive/My Drive/Shared with me/598-58/datasets'\n",
        "\n",
        "\n",
        "# # Or, if you are the owner (brianib2) and shared it with others:\n",
        "# datasets_path = '/content/drive/My Drive/598-58/datasets/brain-tumor-dataset'\n",
        "\n",
        "\n",
        "# import os\n",
        "# import numpy as np\n",
        "# import scipy.io\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# base_path = datasets_path\n",
        "\n",
        "# # Function to load .mat files and extract data\n",
        "# def load_data(folder):\n",
        "#     files = os.listdir(folder)\n",
        "#     data_list = []\n",
        "#     for file in files:\n",
        "#         mat = scipy.io.loadmat(os.path.join(folder, file))\n",
        "#         data = mat['cjdata']\n",
        "#         data_list.append(data)\n",
        "#     return data_list\n",
        "\n",
        "# # Load data from each subset\n",
        "# data_parts = ['brainTumorDataPublic_1766', 'brainTumorDataPublic_7671532',\n",
        "#               'brainTumorDataPublic_15332298', 'brainTumorDataPublic_22993064']\n",
        "# all_data = []\n",
        "# for part in data_parts:\n",
        "#     part_data = load_data(os.path.join(base_path, part))\n",
        "#     all_data.extend(part_data)\n"
      ],
      "metadata": {
        "id": "6ucFI_9DqvC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model\n",
        "\n",
        "The model includes the model definitation which usually is a class, model training, and other necessary parts.\n",
        "\n",
        "- Model architecture: layer number/size/type, activation function, etc\n",
        "- The \"CFG\" class in the code below references both a CNN (ResNet) and a Transformer Model (Vision Transformer (ViT) ), characteristic of the CASS Model\n",
        "- Training objectives: loss function, optimizer, weight of each loss term, etc\n",
        "Others: whether the model is pretrained, Monte Carlo simulation for uncertainty analysis, etc\n",
        "- The code of model should have classes of the model, functions of model training, model validation, etc.\n",
        "- If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it.\n",
        "\n",
        "- NOTE: The citation to the original paper and the original paper's git repo is in the References section below!"
      ],
      "metadata": {
        "id": "M5Bh2TkuH4ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a the number to string maps for labels and the device\n",
        "\n",
        "label_num2str = {\n",
        "    0: \"adipose\",\n",
        "    1: \"background\",\n",
        "    2: \"debris\",\n",
        "    3: \"lymphocytes\",\n",
        "    4: \"mucus\",\n",
        "    5: \"smooth muscle\",\n",
        "    6: \"normal colon mucosa\",\n",
        "    7: \"cancer-associated stroma\",\n",
        "    8: \"colorectal adenocarcinoma epithelium\",\n",
        "}\n",
        "\n",
        "print(f\"label_num2str: {label_num2str}\")\n",
        "\n",
        "label_str2num = {}\n",
        "for i in label_num2str:\n",
        "    label_str2num[label_num2str[i]] = i\n",
        "\n",
        "print(f\"label_str2num: {label_str2num}\")\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"device:\", device)"
      ],
      "metadata": {
        "id": "faULQGTomqSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    # We dont need to give the path to CSV and images as medMNIST provides dataloaders out of hte box.\n",
        "    # Check MNIST Get-started-DEDL.ipynb for details on how to get the label to num, num to label and\n",
        "    # class weights for the MedMNIST dataset.\n",
        "    label_num2str = label_num2str\n",
        "    label_str2num = label_str2num\n",
        "    fl_alpha = 1.0  # alpha of focal_loss\n",
        "    fl_gamma = 2.0  # gamma of focal_loss\n",
        "    cls_weight = [\n",
        "        0.4368473694738948,\n",
        "        0.4597319463892779,\n",
        "        0.5959191838367675,\n",
        "        0.6024804960992198,\n",
        "        0.21920384076815363,\n",
        "        0.8874974994999001,\n",
        "        0.2,\n",
        "        0.4424484896979396,\n",
        "        1.0,\n",
        "    ]\n",
        "    cnn_name = \"resnet50\"\n",
        "    vit_name = \"vit_base_patch16_384\"\n",
        "    seed = 77\n",
        "    num_classes = 9\n",
        "    batch_size = 16\n",
        "    t_max = 16\n",
        "    lr = 1e-3\n",
        "    min_lr = 1e-6\n",
        "    n_fold = 6\n",
        "    num_workers = 8\n",
        "    gpu_idx = 0\n",
        "    device = torch.device(f\"cuda:{gpu_idx}\" if torch.cuda.is_available() else \"cpu\")\n",
        "    gpu_list = [gpu_idx]"
      ],
      "metadata": {
        "id": "4SZXs1gomyAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Focal Loss\n",
        "Focal Loss was chosen by the authors as the loss function because is necessary for addressing class imbalances typically inherent in medical datasets.\n",
        "\n",
        "See: https://medium.com/swlh/focal-loss-an-efficient-way-of-handling-class-imbalance-4855ae1db4cb\n"
      ],
      "metadata": {
        "id": "CvoPUwHJXzIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Define Focal-Loss\n",
        "\n",
        "cls_weights Configuration: Adjust the CFG.cls_weight array to match the frequency of each tumor type in your dataset.\n",
        "This should reflect the inverse frequency of each class or other heuristics that you find suitable for balancing the class influence in loss calculation.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    The focal loss for fighting against class-imbalance\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alpha=1, gamma=2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = 1e-12  # prevent training from Nan-loss error\n",
        "        self.cls_weights = torch.tensor(\n",
        "            [CFG.cls_weight], dtype=torch.float, requires_grad=False, device=CFG.device\n",
        "        )\n",
        "\n",
        "    def forward(self, logits, target):\n",
        "        \"\"\"\n",
        "        logits & target should be tensors with shape [batch_size, num_classes]\n",
        "        \"\"\"\n",
        "        probs = torch.sigmoid(logits)\n",
        "        one_subtract_probs = 1.0 - probs\n",
        "        # add epsilon\n",
        "        probs_new = probs + self.epsilon\n",
        "        one_subtract_probs_new = one_subtract_probs + self.epsilon\n",
        "        # calculate focal loss\n",
        "        log_pt = target * torch.log(probs_new) + (1.0 - target) * torch.log(\n",
        "            one_subtract_probs_new\n",
        "        )\n",
        "        pt = torch.exp(log_pt)\n",
        "        focal_loss = -1.0 * (self.alpha * (1 - pt) ** self.gamma) * log_pt\n",
        "        focal_loss = focal_loss * self.cls_weights\n",
        "        return torch.mean(focal_loss)"
      ],
      "metadata": {
        "id": "JsD50jLSnAer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Define F1 score metric\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class MyF1Score(Metric):\n",
        "    def __init__(self, cfg, threshold: float = 0.5, dist_sync_on_step=False):\n",
        "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
        "        self.cfg = cfg\n",
        "        self.threshold = threshold\n",
        "        self.add_state(\"tp\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
        "        self.add_state(\"fp\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
        "        self.add_state(\"fn\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
        "\n",
        "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
        "        # assert preds.shape == target.shape\n",
        "        preds_str_batch = self.num_to_str(torch.sigmoid(preds))\n",
        "        target_str_batch = self.num_to_str(target)\n",
        "        tp, fp, fn = 0, 0, 0\n",
        "        for pred_str_list, target_str_list in zip(preds_str_batch, target_str_batch):\n",
        "            for pred_str in pred_str_list:\n",
        "                if pred_str in target_str_list:\n",
        "                    tp += 1\n",
        "                if pred_str not in target_str_list:\n",
        "                    fp += 1\n",
        "\n",
        "            for target_str in target_str_list:\n",
        "                if target_str not in pred_str_list:\n",
        "                    fn += 1\n",
        "        self.tp += tp\n",
        "        self.fp += fp\n",
        "        self.fn += fn\n",
        "\n",
        "    def compute(self):\n",
        "        # To switch between F1 score and recall.\n",
        "        f1 = 2.0 * self.tp / (2.0 * self.tp + self.fn + self.fp)\n",
        "        rec = self.tp / (self.tp + self.fn)\n",
        "        return f1\n",
        "\n",
        "    def num_to_str(self, ts: torch.Tensor) -> list:\n",
        "        batch_bool_list = (ts > self.threshold).detach().cpu().numpy().tolist()\n",
        "        batch_str_list = []\n",
        "        for one_sample_bool in batch_bool_list:\n",
        "            # print(self.cfg.label_num2str)\n",
        "            lb_str_list = [\n",
        "                self.cfg.label_num2str[lb_idx]\n",
        "                for lb_idx, bool_val in enumerate(one_sample_bool)\n",
        "                if bool_val\n",
        "            ]\n",
        "            batch_str_list.append(lb_str_list)\n",
        "        return batch_str_list"
      ],
      "metadata": {
        "id": "TmcXu9QAnRa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "cfg=CFG()\n",
        "model_cnn = timm.create_model(cfg.cnn_name, pretrained=True)\n",
        "model_vit = timm.create_model(cfg.vit_name, pretrained=True)\n",
        "model_cnn.to(device)\n",
        "model_vit.to(device)"
      ],
      "metadata": {
        "id": "mIFwVWTKni_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.cuda as cuda\n",
        "\n",
        "\n",
        "def ssl_train_model(\n",
        "    train_loader,\n",
        "    model_vit,\n",
        "    optimizer_vit,\n",
        "    scheduler_vit,\n",
        "    model_cnn,\n",
        "    optimizer_cnn,\n",
        "    scheduler_cnn,\n",
        "    num_epochs,\n",
        "):\n",
        "    writer = SummaryWriter()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    phase = \"train\"\n",
        "    model_cnn.train()\n",
        "    model_vit.train()\n",
        "\n",
        "    # print(cuda.memory_summary(device=device, abbreviated=False))\n",
        "\n",
        "    for i in tqdm(range(num_epochs)):\n",
        "        with torch.set_grad_enabled(phase == \"train\"):\n",
        "            for img, _ in tqdm(train_loader):\n",
        "                img = img.to(device)\n",
        "\n",
        "                # use for debugging\n",
        "                # print(f\"Start of loop - {cuda.memory_summary(device=device, abbreviated=True)}\")\n",
        "                pred_vit = model_vit(img)\n",
        "                pred_cnn = model_cnn(img)\n",
        "\n",
        "                model_sim_loss = loss_fn(pred_vit, pred_cnn)\n",
        "                loss = model_sim_loss.mean()\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                optimizer_cnn.step()\n",
        "                optimizer_vit.step()\n",
        "                scheduler_cnn.step()\n",
        "                scheduler_vit.step()\n",
        "\n",
        "                print(\"For -\", i, \"Loss:\", loss.item())\n",
        "                writer.add_scalar(\"Self-Supervised Loss/train\", loss.item(), i)\n",
        "\n",
        "                del img, pred_vit, pred_cnn, model_sim_loss, loss\n",
        "\n",
        "    writer.flush()\n",
        "\n",
        "\n",
        "def loss_fn(x, y):\n",
        "    x = torch.nn.functional.normalize(x, dim=-1, p=2)\n",
        "    y = torch.nn.functional.normalize(y, dim=-1, p=2)\n",
        "    return 2 - 2 * (x * y).sum(dim=-1)"
      ],
      "metadata": {
        "id": "TAHyQGt5oobm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_cnn = SWA(torch.optim.Adam(model_cnn.parameters(), lr=1e-3))\n",
        "optimizer_vit = SWA(torch.optim.Adam(model_vit.parameters(), lr=1e-3))\n",
        "scheduler_cnn = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer_cnn, T_max=16, eta_min=1e-6\n",
        ")\n",
        "scheduler_vit = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer_vit, T_max=16, eta_min=1e-6\n",
        ")\n",
        "\n",
        "fl_alpha = 1.0  # alpha of focal_loss\n",
        "fl_gamma = 2.0  # gamma of focal_loss\n",
        "cls_weight = [0.9475164011246484, 0.4934395501405811, 0.5029053420805999, 0.2, 1.0]\n",
        "\n",
        "# these are unused (brian 4/11)\n",
        "# criterion_vit = FocalLoss(fl_alpha, fl_gamma)\n",
        "# criterion_cnn = FocalLoss(fl_alpha, fl_gamma)"
      ],
      "metadata": {
        "id": "-xkmiU8to3cA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n"
      ],
      "metadata": {
        "id": "8-KX8ez9w6uP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Computational Requirements\n",
        "* We are able to get through a clean run of self supervised training using the V100 GPU on the Google Colab Notebook (this required a paid Colab Pro account).\n",
        "\n",
        "However, both the CNN and Transformer portions of CASS came with additional unforeseen issues which forced us to invent workarounds and other modifications detailed below.\n",
        "\n",
        "\n",
        "- We are also able to run through the supervised CNN training although this part of the code still needs modifications to work as expected.\n",
        "\n",
        "- Finally, when training the ViT model we run into a CUDA memory error. It is worth noting that **significant** adjustments had to be made to even get to this point where we could successfully complete self supervised training.\n",
        "  - We sampled a subset of the medMNIST dataset, only trained for 1 epoch, and lowered batch size to 8.\n",
        "\n",
        "In comparison, the CASS authors used a batch size of 16 and 100 epochs during self-supervised learning.\n",
        "\n"
      ],
      "metadata": {
        "id": "9ccbP5jTZwY0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Requirement             | Description                                                 |\n",
        "|------------------------|-------------------------------------------------------------|\n",
        "| Hardware               | NVIDA V100 GPU (Google Colab)       |\n",
        "|                        \n",
        "| Average Runtime per Epoch | - Average runtime for each epoch (e.g., in minutes)        |\n",
        "| Total Trials           | - Total number of trials (e.g., hyperparameter tuning)      |\n",
        "| GPU Hours Used         | See Comment |\n",
        "| Training Epochs        | - Total number of training epochs                            |\n",
        "| Other Requirements     | See `Data` and `Model` sections of this notebook for library and parameter requirements. |\n"
      ],
      "metadata": {
        "id": "AOqNTPmMb9Re"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we perform both self=supervised training (CASS) and supervised training via CNN and ViT\n"
      ],
      "metadata": {
        "id": "7KPxLVeIZ3nj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train using self-supervised learning\n",
        "import os\n",
        "\n",
        "print(f'Number of training samples: {len(train_loader)}')\n",
        "\n",
        "# Check if checkpoints exist\n",
        "# These are the location of the paths from the shared drive\n",
        "checkpoints_directory = \"/content/drive/MyDrive/598-58/checkpoints\"\n",
        "checkpoint_files = os.listdir(checkpoints_directory)\n",
        "\n",
        "print(f'checkpoint files: {checkpoint_files}')\n",
        "\n",
        "failed_to_load = False\n",
        "if \"cass-r50-med-mnist-pathmnist.pt\" in checkpoint_files:\n",
        "    print(\"Loading ResNet50 model\")\n",
        "    model_cnn = torch.load(f'{checkpoints_directory}/cass-r50-med-mnist-pathmnist.pt')\n",
        "else:\n",
        "    failed_to_load = True\n",
        "\n",
        "if \"cass-vit-med-mnist-pathmnist.pt\" in checkpoint_files:\n",
        "    print(\"Loading ViT model\")\n",
        "    model_vit = torch.load(f'{checkpoints_directory}/cass-vit-med-mnist-pathmnist.pt')\n",
        "else:\n",
        "    failed_to_load = True\n",
        "\n",
        "if failed_to_load:\n",
        "    print('Running self-')\n",
        "    ssl_train_model(\n",
        "        train_loader,\n",
        "        model_vit,\n",
        "        optimizer_vit,\n",
        "        scheduler_vit,\n",
        "        model_cnn,\n",
        "        optimizer_cnn,\n",
        "        scheduler_cnn,\n",
        "        num_epochs=1,\n",
        "    )\n",
        "\n",
        "  # Saving SSL Models\n",
        "  print(\"Saving Cov-T\")\n",
        "\n",
        "  torch.save(model_cnn, \"./cass-r50-med-mnist-pathmnist.pt\")\n",
        "  torch.save(model_vit, \"./cass-vit-med-mnist-pathmnist.pt\")"
      ],
      "metadata": {
        "id": "kK1qcYgMo89p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train using supervised learning (need labels)\n",
        "\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "# TODO Commented out line below because indenting was off for a for loop and did not see it used at all\n",
        "# for fold_idx, (train_indices, valid_indices) in enumerate(k_fold.split(all_img_names)):\n",
        "model_vit = torch.load(\"./cass-vit-med-mnist-pathmnist.pt\")\n",
        "model_cnn = torch.load(\"./cass-r50-med-mnist-pathmnist.pt\")\n",
        "last_loss = math.inf\n",
        "val_loss_arr = []\n",
        "train_loss_arr = []\n",
        "counter = 0\n",
        "\n",
        "model_cnn.to(device)\n",
        "model_vit.to(device)\n",
        "print(\"*\" * 10)\n",
        "\n",
        "\n",
        "# Train Correspong Supervised CNN\n",
        "print(\"Fine tunning Cov-T\")\n",
        "writer = SummaryWriter()\n",
        "model_cnn.fc = nn.Linear(in_features=2048, out_features=9, bias=True)\n",
        "criterion = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)\n",
        "metric = MyF1Score(cfg)\n",
        "val_metric = MyF1Score(cfg)\n",
        "optimizer = torch.optim.Adam(model_cnn.parameters(), lr=3e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer, T_max=cfg.t_max, eta_min=cfg.min_lr\n",
        ")\n",
        "model_cnn.train()\n",
        "from torch.autograd import Variable\n",
        "\n",
        "best = 0\n",
        "best_val = 0\n",
        "for epoch in tqdm(range(1)):\n",
        "    total_loss = 0\n",
        "    for images, label in tqdm(train_loader):\n",
        "        model_cnn.train()\n",
        "        images = images.to(device)\n",
        "        label = label.to(device)\n",
        "        model_cnn.to(device)\n",
        "        pred_ts = model_cnn(images)\n",
        "        loss = criterion(pred_ts, label)\n",
        "        score = metric(pred_ts, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        scheduler.step()\n",
        "        total_loss += loss.detach()\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    train_score = metric.compute()\n",
        "    logs = {\n",
        "        \"train_loss\": avg_loss,\n",
        "        \"train_f1\": train_score,\n",
        "        \"lr\": optimizer.param_groups[0][\"lr\"],\n",
        "    }\n",
        "    writer.add_scalar(\"CNN Supervised Loss/train\", loss, epoch)\n",
        "    writer.add_scalar(\"CNN Supervised F1/train\", train_score, epoch)\n",
        "    print(logs)\n",
        "\n",
        "    if best < train_score:\n",
        "        best = train_score\n",
        "        model_cnn.eval()\n",
        "        total_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for images, label in valid_loader:\n",
        "                images = images.to(device)\n",
        "                label = label.to(device)\n",
        "                model_cnn.to(device)\n",
        "                pred_ts = model_cnn(images)\n",
        "                score_val = val_metric(pred_ts, label)\n",
        "                val_loss = criterion(pred_ts, label)\n",
        "                total_loss += val_loss.detach()\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(\"Val Loss:\", avg_loss)\n",
        "        val_score = val_metric.compute()\n",
        "        print(\"CNN Validation Score:\", val_score)\n",
        "        writer.add_scalar(\"CNN Supervised F1/Validation\", val_score, epoch)\n",
        "        if avg_loss > last_loss:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = 0\n",
        "\n",
        "        last_loss = avg_loss\n",
        "        if counter > 5:\n",
        "            print(\"Early Stopping!\")\n",
        "            break\n",
        "        else:\n",
        "            if val_score > best_val:\n",
        "                best_val = val_score\n",
        "                print(\"Saving\")\n",
        "                torch.save(model_cnn, \"./cass-r50-med-mnist-pathmnist-label.pt\")\n",
        "writer.flush()\n",
        "last_loss = 999999999\n",
        "val_loss_arr = []\n",
        "train_loss_arr = []\n",
        "counter = 0\n",
        "\n",
        "\n",
        "# Training the Corresponding ViT\n",
        "model_vit.head = nn.Linear(in_features=768, out_features=9, bias=True)\n",
        "criterion = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)\n",
        "metric = MyF1Score(cfg)\n",
        "optimizer = torch.optim.Adam(model_vit.parameters(), lr=3e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer, T_max=cfg.t_max, eta_min=cfg.min_lr\n",
        ")\n",
        "model_vit.train()\n",
        "val_metric = MyF1Score(cfg)\n",
        "writer = SummaryWriter()\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "best = 0\n",
        "best_val = 0\n",
        "for epoch in tqdm(range(1)):\n",
        "    total_loss = 0\n",
        "    for images, label in tqdm(train_loader):\n",
        "        model_vit.train()\n",
        "        images = images.to(device)\n",
        "        label = label.to(device)\n",
        "        model_vit.to(device)\n",
        "        pred_ts = model_vit(images)\n",
        "        loss = criterion(pred_ts, label)\n",
        "        score = metric(pred_ts, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        scheduler.step()\n",
        "        total_loss += loss.detach()\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    train_score = metric.compute()\n",
        "    logs = {\n",
        "        \"train_loss\": loss,\n",
        "        \"train_f1\": train_score,\n",
        "        \"lr\": optimizer.param_groups[0][\"lr\"],\n",
        "    }\n",
        "    writer.add_scalar(\"ViT Supervised Loss/train\", loss, epoch)\n",
        "    writer.add_scalar(\"ViT Supervised F1/train\", train_score, epoch)\n",
        "    print(logs)\n",
        "    if best < train_score:\n",
        "        best = train_score\n",
        "        model_vit.eval()\n",
        "        total_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for images, label in valid_loader:\n",
        "                images = images.to(device)\n",
        "                label = label.to(device)\n",
        "                model_vit.to(device)\n",
        "                pred_ts = model_vit(images)\n",
        "                score_val = val_metric(pred_ts, label)\n",
        "                val_loss = criterion(pred_ts, label)\n",
        "                total_loss += val_loss.detach()\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        val_score = val_metric.compute()\n",
        "        print(\"ViT Validation Score:\", val_score)\n",
        "        print(\"Val Loss:\", avg_loss)\n",
        "        writer.add_scalar(\"ViT Supervised F1/Validation\", val_score, epoch)\n",
        "        if avg_loss > last_loss:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = 0\n",
        "\n",
        "        last_loss = avg_loss\n",
        "        if counter > 5:\n",
        "            print(\"Early Stopping!\")\n",
        "            break\n",
        "        else:\n",
        "            if val_score > best_val:\n",
        "                best_val = val_score\n",
        "                print(\"Saving\")\n",
        "                torch.save(model_vit, \"./cass-vit-med-mnist-pathmnist.pt\")\n",
        "\n",
        "    writer.flush()\n",
        "    print(\"*\" * 10)"
      ],
      "metadata": {
        "id": "pc1jajc8t7mC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n",
        "\n",
        "We are still working out the final stages of our training section, specifically the supervised learning that follows the initial self-supervised representation learning via CASS. Therefore, we are lacking in metrics right now. However, you can see above in the self-supervised training output that we progressively get lower and lower loss values as we step through the self supervised learning process for our training dataset. We will continue working to get a model trained so that we can run evaluations and present some data to discuss."
      ],
      "metadata": {
        "id": "30FQzJsZw7tM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class my_model():\n",
        "  # use this class to define your model\n",
        "  pass\n",
        "\n",
        "model = my_model()\n",
        "loss_func = None\n",
        "optimizer = None\n",
        "\n",
        "def train_model_one_iter(model, loss_func, optimizer):\n",
        "  pass\n",
        "\n",
        "num_epoch = 10\n",
        "# model training loop: it is better to print the training/validation losses during the training\n",
        "for i in range(num_epoch):\n",
        "  train_model_one_iter(model, loss_func, optimizer)\n",
        "  train_loss, valid_loss = None, None\n",
        "  print(\"Train Loss: %.2f, Validation Loss: %.2f\" % (train_loss, valid_loss))\n"
      ],
      "metadata": {
        "id": "gBdVZoTvsSFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "\n",
        "This section will contain statistical results from running CASS using the MedMNIST dataset.\n",
        "\n",
        "* Because the Brain Tumor dataset was previously the only dataset the authors used which we could find, and because this dataset ultimately proved to somehow not be the same data that the authors describe, we were not able to implement CASS-trained CNN and Transformers using this dataset and there are no results to present for this dataset.\n",
        "* Results (medMNIST): For the medMNIST dataset, we were able to successfully reproduce CASS for representational learning. There were many difficulties along the way, mainly due to computational requirements for the model training and limitations in computing resources. The GitHub repo for the paper was also not perfect and we had to make many adjustments to get the code to compile and run with successful results. We are not able to overcome these obstacles yet for the supervised learning component.\n",
        "*Analysis (medMNIST): The output throughout the notebook above shows evidence of EDA, model definitions, and training results. As mentioned above in evaluation, the loss values for CASS in the training output show successful completion for one epoch. The self-supervised portion of this is still in work as we ran into computational hurdles and complications in reproducing the models.\n",
        "*Plans (medMNIST): Our plans for the next phase begin with completion of supervised learning. We will then consider our ablation studies. In doing both of these we will have to work through the computational limitations presented. We may also look to get the implementation working with a second dataset (Brain Tumor), which has EDA shown above."
      ],
      "metadata": {
        "id": "gX6bCcZNuxmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# metrics to evaluate my model\n",
        "\n",
        "# plot figures to better show the results\n",
        "\n",
        "# it is better to save the numbers and figures for your presentation."
      ],
      "metadata": {
        "id": "LjW9bCkouv8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Metric       | Your Model | Author's Model | Dataset Sample Size |\n",
        "|--------------|------------|----------------|---------------------|\n",
        "| Accuracy     | XX.X%      | XX.X%          | N                   |\n",
        "| AUC          | X.XX       | X.XX           | N                   |\n",
        "| RMSE         | X.XX       | X.XX           | N                   |\n",
        "| Precision    | XX.X%      | XX.X%          | N                   |\n",
        "| Recall       | XX.X%      | XX.X%          | N                   |\n",
        "| F1 Score     | X.XX       | X.XX           | N                   |\n",
        "| Sensitivity  | XX.X%      | XX.X%          | N                   |\n",
        "| Specificity  | XX.X%      | XX.X%          | N                   |\n",
        "\n",
        "**Notes:**\n",
        "- `N` is the total number of samples in the dataset used for testing.\n",
        "- `XX.X%` and `X.XX` represent placeholder values for the actual percentages and scores respectively.\n",
        "\n"
      ],
      "metadata": {
        "id": "zHvm4I7lem0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Findings (vs. Authors' Original Paper)"
      ],
      "metadata": {
        "id": "hCzhFECXe76a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compare you model with others\n",
        "# you don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper"
      ],
      "metadata": {
        "id": "uOdhGrbwwG71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model comparison"
      ],
      "metadata": {
        "id": "8EAWAy_LwHlV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ablation Study\n",
        "\n",
        "As stated in our **Scope of Reproducibility** section, our second hypothesis was that reducing the number of pre-training epochs and batch sizes for the CASS model will still allow for strong model performance in comparison with existing methods.\n",
        "\n",
        "However, given our significant difficulties in attempting to reproduce the authors findings due to:\n",
        "- Lack of access to their original datasets\n",
        "- Lack of documentation and \"out of the box\" functionality in authors' starter code\n",
        "- Lack of access to equivalent computing resources\n",
        "\n",
        "We were unable to test this hypothesis, and as such the ablation study remains to be performed. We hope that other teams and groups may succeed in testing this hypothesis and sharing their findings.\n",
        "\n",
        "To some extent, by needing to adjust parameters such as training epochs and batch sizes in order to train the model, we performed an ablation study of sorts. However, we acknowledge that the intention of the study is to expound upon the original paper's findings and explore variations in implementation or usage."
      ],
      "metadata": {
        "id": "1Tu3HI-Wd0wI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "Generally, Team 58 found recreating the CASS experiments from the research paper to be extremely difficult for a multitude of reasons which we will explain in this section. As such, we were not able to replicate the study, much less experiment with our proposed ablations.\n",
        "\n",
        "\n",
        "In this section,you should discuss your work and make future plan. The discussion should address the following questions:\n",
        "  * Make assessment that the paper is reproducible or not.\n",
        "  * Explain why it is not reproducible if your results are kind negative.\n",
        "  * Describe “What was easy” and “What was difficult” during the reproduction.\n",
        "  * Make suggestions to the author or other reproducers on how to improve the reproducibility.\n",
        "  * What will you do in next phase.\n",
        "\n"
      ],
      "metadata": {
        "id": "qH75TNU71eRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our results demonstrate that the paper is/is not reproducible. This conclusion is based on the following key observations:\n",
        "1. This is a key observation\n",
        "2. This is a key observation\n",
        "3. This is a key observation\n",
        "\n",
        "It is important to note here that here is a bunch of filler text to swap out with unique relevancy of observations. It is reproducible in that here are the ways we demonstrated its' reproducibility, however here are some other reasons why some unexpected things happened which we could not have anticipated. In the future, we would do some of these other things differently given appropriate resources in terms of time/money/knowledge/people.\n",
        "\n",
        "The easiest part of reproducing this paper was this things right here and some other stuff there. This was or was not easy based on our initial assessment of the required tasks.\n",
        "\n",
        "The most difficult part was this thing here. We did or did not anticipate the difficulty of this, and would advise other groups in the future to consider this carefully. Here's how it might be mitigated.\n"
      ],
      "metadata": {
        "id": "KfY6ljIUcqm_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Availability (or lack thereof)\n",
        "\n",
        "As described in our `Methodology` and `Data` sections, of the datasets referenced by the authors, only one (Brain Tumor dataset) was publicly available.\n",
        "\n",
        "Given that, upon exploratory data analysis (EDA) we discovered that the dataset did not contain the same number of total records as the one the researchers used, and further given that this dataset included an even further skewed class imbalance (closer to 90/10 than the 80/20 described by the CASS authors), we determined that we could not faithfully recreate the research experiments using this dataset.\n",
        "\n",
        "This left us to use the MedMNIST dataset which was not described in the paper, but was referenced in the authors' Github starter code and which we proposed might be suitable for our hypothesis."
      ],
      "metadata": {
        "id": "aD7AG_0aJy80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Starter Code from Researchers\n",
        "\n",
        "We found the code available to use to be poorly documented. There were two different CASS.ipynb notebooks to use, and the \"Getting Started\" notebook provided insufficient details for getting CASS up and running using the authors' own code and provided dataset (MedMNIST), let alone tailoring this implementation to other datasets.\n",
        "\n",
        "In the case of both the MedMNIST and brain tumor datasets, we found ourselves spending in excess of 50 hours attempting to troubleshoot bugs in the given code, or attempting to understand the authors' particular approach to implementing a CNN and Transformer trained with CASS. Factoring into all of this that the brain tumor dataset is in greyscale while the MedMNIST is in color, we found ourselves consumed by \"learning on the fly\" and attempting to troubleshoot errors rather than recreating a study."
      ],
      "metadata": {
        "id": "up607V0oJ3UN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Computing Limitations\n",
        "\n",
        "One of the benefits of CASS espoused by the authors in their paper is the ability to overcome a lack of computing resources. However, we found this \"advantage\" to be ironic in that all three of our team members resorted to cloning our test notebook to personal Google accounts and upgrading to paid Google Collab subscriptions in order to buy higher-capacity computing resources than those offered for free with our educational accounts.\n",
        "\n",
        "The authors utilized an NVIDIA RTX8000 graphical processing unit (GPU) with 48 GB video RAM, 2 PU cores, and 64 GB system RAM.\n",
        "\n",
        "Team 58 utilized at any given time [10]:\n",
        "- NVIDIA Tesla T4 GPU with 16 GB RAM, unknown CPU (system) RAM\n",
        "- NVIDA T100 GPU with 16GB RAM, unknown CPU (system) RAM\n",
        "- NVIDIA A100 GPU with 80GB RAM, unknown CPU (system) RAM\n",
        "- Mac M1 Silicon CPU\n",
        "\n",
        "We frequently found ourselves hitting consumption limits on the Tesla T4 GPU that would boot us from the notebook for the remainder of the day. Paid GPU instances still required us to limit batch sizes in order for the models to run through a single epoch without freezing or crashing.\n",
        "\n",
        "That we paid a combined amount of at least $30 USD and still were unable to faithfully recreate the authors' work stands as a testament to the difficulty in reproducing this paper."
      ],
      "metadata": {
        "id": "FNOr4s1XKlVh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Suggestions for other teams\n",
        "\n",
        "We recommend evaluating your computing resources ahead of time in order to be prepared for the challenges in training the self-supervised and supervised models. We also recommend becoming as familiar with the model as possible before starting to reproduce, because the existing code has many flaws and requires modifications to get working."
      ],
      "metadata": {
        "id": "kBV6l-v6JuED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Suggestions for authors\n",
        "\n",
        "We recommend revising the code base in GitHub as there were many issues to resolve, from basics like installs and imports to the model and training code itself. We also recommend providing more detailed info on accessing and preprocessing the datasets mentioned in the paper as we ended up using medMNIST which was only mentioned in the git repo."
      ],
      "metadata": {
        "id": "rJEdw_VPylQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Public GitHub Repo\n",
        "Please see introduction section above for link to team GitHub repo."
      ],
      "metadata": {
        "id": "Omx8ERfc2a1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "1. Singh, P. & Cirrone, J. (2023). Efficient Representation Learning for Healthcare with Cross-Architectural Self-Supervision. Proceedings of the 8th Machine Learning for Healthcare Conference, in Proceedings of Machine Learning Research, 219, 691-711. Available from https://proceedings.mlr.press/v219/singh23a.html.\n",
        "\n",
        "2. Singh, P. (2022). Official PyTorch implementation of CASS. [Software]. Retrieved from https://github.com/pranavsinghps1/CASS.\n",
        "\n",
        "3. Cheng, J. (2017, April 2). Brain tumor dataset. figshare. Retrieved from https://figshare.com/articles/dataset/brain_tumor_dataset/1512427.\n",
        "\n",
        "4. Tschandl P., Rosendahl C., & Kittler H. (2018). The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. Sci. Data, 5, 180161. https://doi.org/10.1038/sdata.2018.161.\n",
        "\n",
        "5. Codella, N. C. F., Gutman, D., Celebi, M. E., Helba, B., Marchetti, M. A., Dusza, S. W., Kalloo, A., Liopyris, K., Mishra, N., Kittler, H., & Halpern, A. (2017). Skin Lesion Analysis Toward Melanoma Detection: A Challenge at the 2017 International Symposium on Biomedical Imaging (ISBI), Hosted by the International Skin Imaging Collaboration (ISIC). arXiv:1710.05006. Retrieved from https://arxiv.org/abs/1710.05006.\n",
        "\n",
        "6. Combalia, M., Codella, N. C. F., Rotemberg, V., Helba, B., Vilaplana, V., Reiter, O., Halpern, A. C., Puig, S., & Malvehy, J. (2019). BCN20000: Dermoscopic Lesions in the Wild. arXiv:1908.02288. Retrieved from https://arxiv.org/abs/1908.02288.\n",
        "\n",
        "7. Yang, J., Shi, R., Wei, D., Liu, Z., Zhao, L., Ke, B., Pfister, H., & Ni, B. (2023). MedMNIST v2-A large-scale lightweight benchmark for 2D and 3D biomedical image classification. Scientific Data.\n",
        "\n",
        "8. Yang, J., Shi, R., & Ni, B. (2021). MedMNIST Classification Decathlon: A Lightweight AutoML Benchmark for Medical Image Analysis. IEEE 18th International Symposium on Biomedical Imaging (ISBI).\n",
        "\n",
        "9. Bhuvaji, S. (2020). Brain Tumor Classification Using Deep Learning Algorithms. Retrieved from https://github.com/SartajBhuvaji/Brain-Tumor-Classification-Using-Deep-Learning-Algorithms.\n",
        "\n",
        "10. Google. (2024). Google Cloud Compute Specifications. Retrieved from https://cloud.google.com/compute/docs/gpus.\n"
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2W155rxjMeaH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}